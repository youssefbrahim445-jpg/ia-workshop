{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264fa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da252638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting groq\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting sniffio (from groq)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.10 (from groq)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->groq)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, idna, h11, distro, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, groq\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 distro-1.9.0 groq-0.31.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 typing-extensions-4.15.0 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c6c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: groq in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (0.31.0)\n",
      "Collecting dotenv\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from groq) (4.15.0)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pc-lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install groq dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead25182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have revolutionized the field of natural language processing (NLP) by providing efficient and accurate text analysis capabilities. These models are designed to process and understand human language at unprecedented speeds, making them essential for various applications.\n",
      "\n",
      "Here are some key reasons why fast language models are important:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable real-time text analysis, which is crucial for applications like chatbots, virtual assistants, and language translation software. This allows for seamless interactions between humans and machines, enhancing the overall user experience.\n",
      "2. **Increased Efficiency**: By processing large volumes of text data quickly, fast language models can automate tasks like sentiment analysis, entity recognition, and text classification. This automation saves time and resources, making businesses and organizations more efficient.\n",
      "3. **Enhanced Accuracy**: Fast language models can analyze vast amounts of text data, which enables them to learn patterns and relationships that might be missed by human analysts. This leads to more accurate insights and better decision-making.\n",
      "4. **Real-time Insights**: Fast language models can provide instant feedback and insights, allowing businesses to respond promptly to customer feedback, market trends, and emerging issues.\n",
      "5. **Scalability**: Fast language models can handle large volumes of text data, making them ideal for applications that require processing vast amounts of information, such as social media monitoring, customer service, and content analysis.\n",
      "6. **Research and Development**: Fast language models accelerate research in NLP, enabling scientists to explore new ideas, test hypotheses, and develop innovative applications more quickly.\n",
      "7. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive edge by responding faster to changing market conditions, improving customer engagement, and optimizing their operations.\n",
      "\n",
      "Some of the notable applications of fast language models include:\n",
      "\n",
      "* Sentiment analysis and opinion mining\n",
      "* Chatbots and virtual assistants\n",
      "* Language translation and localization\n",
      "* Text classification and clustering\n",
      "* Named entity recognition and information extraction\n",
      "* Question answering and summarization\n",
      "\n",
      "In summary, fast language models have transformed the field of NLP by providing efficient, accurate, and scalable text analysis capabilities. Their importance lies in their ability to improve user experience, increase efficiency, enhance accuracy, and provide real-time insights, making them a crucial component of various applications and industries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "            \"content\":\"you are an incpert in geography dont ask any question about geography \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8b8e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are a crucial component of modern natural language processing (NLP) systems, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models enable the efficient processing of large amounts of text data, which is essential for many NLP applications, such as text classification, sentiment analysis, and machine translation. By reducing the computational time and resources required for processing, fast language models make it possible to handle large volumes of data in real-time.\n",
      "2. **Improved User Experience**: Fast language models can significantly enhance the user experience in various applications, such as:\n",
      "\t* **Virtual Assistants**: Quick response times allow virtual assistants like Siri, Google Assistant, or Alexa to provide instant answers to user queries, making them more interactive and engaging.\n",
      "\t* **Language Translation**: Fast language models facilitate real-time translation, enabling users to communicate across languages more efficiently.\n",
      "\t* **Text Summarization**: Rapid text summarization helps users quickly grasp the main ideas and key points of a document, saving time and effort.\n",
      "3. **Scalability and Deployment**: Fast language models are essential for deploying NLP models in resource-constrained environments, such as:\n",
      "\t* **Edge Devices**: Fast models can run on edge devices like smartphones, smart home devices, or autonomous vehicles, where computational resources are limited.\n",
      "\t* **Cloud Services**: Efficient language models reduce the computational requirements for cloud-based services, making them more scalable and cost-effective.\n",
      "4. **Real-Time Applications**: Fast language models are vital for real-time applications, such as:\n",
      "\t* **Chatbots**: Quick response times enable chatbots to engage in conversations, providing instant support and assistance to customers.\n",
      "\t* **Sentiment Analysis**: Fast models can analyze user feedback and sentiment in real-time, helping businesses respond promptly to customer concerns.\n",
      "\t* **Content Moderation**: Efficient language models facilitate real-time content moderation, ensuring that online platforms can quickly identify and remove objectionable content.\n",
      "5. **Resource Optimization**: Fast language models can optimize resource utilization, leading to:\n",
      "\t* **Reduced Energy Consumption**: By minimizing computational requirements, fast models can help reduce energy consumption and lower the carbon footprint of NLP systems.\n",
      "\t* **Cost Savings**: Efficient models can decrease the costs associated with deploying and maintaining NLP systems, making them more accessible to a broader range of organizations and individuals.\n",
      "6. **Research and Development**: Fast language models accelerate research and development in the field of NLP, allowing researchers to:\n",
      "\t* **Explore New Ideas**: Quick experimentation and prototyping enable researchers to explore new ideas and approaches more efficiently.\n",
      "\t* **Train Larger Models**: Efficient models facilitate the training of larger and more complex models, which can lead to breakthroughs in NLP research.\n",
      "\n",
      "In summary, fast language models are essential for efficient processing, improved user experience, scalability, real-time applications, resource optimization, and research and development in the field of NLP.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d13b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "dotenv.load_dotenv()\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "speech_file_path = \"speech.wav\" \n",
    "model = \"playai-tts\"\n",
    "voice = \"Fritz-PlayAI\"\n",
    "text = \"the capital of mozambique\"\n",
    "response_format = \"wav\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=model,\n",
    "    voice=voice,\n",
    "    input=text,\n",
    "    response_format=response_format\n",
    ")\n",
    "\n",
    "response.write_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fd1a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2359130024.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfilename =\"C:\\Users\\pc-lenovo\\OneDrive\\Documents\\Enregistrement audio.wav\" # Replace with your audio file!\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq()\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename =\"C:\\Users\\pc-lenovo\\OneDrive\\Documents\\Enregistrements audio.wav\" # Replace with your audio file!\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=file, # Required audio file\n",
    "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"verbose_json\",  # Optional\n",
    "      timestamp_granularities = [\"word\", \"segment\"], # Optional (must set response_format to \"json\" to use and can specify \"word\", \"segment\" (default), or both)\n",
    "      language=\"en\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # To print only the transcription text, you'd use print(transcription.text) (here we're printing the entire transcription object to access timestamps)\n",
    "    print(json.dumps(transcription, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
